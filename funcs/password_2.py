import osfrom typing import Iterable, List, Optionalfrom pathlib import Path#Bebug switcher(default off)DEBUG_PASSWORD2 = os.getenv("DEBUG_PASSWORD2", "0") == "1"RUN_EMBEDDING_SELFTEST = os.getenv("RUN_EMBEDDING_SELFTEST", "1") == "1"#Loading .env(before getenv)ENV_ROOT = Path(__file__).resolve().parents[1] #Root directoryENV_FILE = ENV_ROOT / ".env"try:    from dotenv import load_dotenv,dotenv_values    _file_vars = dotenv_values(ENV_FILE)    if DEBUG_PASSWORD2:        print("[debug] .env path:", str(ENV_FILE), "exists:", ENV_FILE.exists())        print("[debug] .env has DEEPSEEK_API_KEY:", bool(_file_vars.get("DEEPSEEK_API_KEY")))        print("[debug] .env has VOLC_API_KEY    :", bool(_file_vars.get("VOLC_API_KEY")))    #write  .env into process environment    load_dotenv(ENV_FILE,override=True)except Exception as e:    if DEBUG_PASSWORD2:        print("[debug] dotenv load skipped:", e)    passfrom openai import OpenAItry:    from httpx import Timeout                                              # 【新增】except Exception:    Timeout = Nonedef _getenv_any(*names):    """按给定的多个名字依次取第一个非空值"""    for n in names:        v = os.getenv(n)        if v:            return v    return None# ====== 环境变量读取 ======# 聊天（DeepSeek 或你自己的兼容 OpenAI 的网关）DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY")DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")DEEPSEEK_MODEL    = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")# 向量（火山方舟 Ark / Doubao）VOLC_API_KEY   = _getenv_any("VOLC_API_KEY","ARK_API_KEY","VOLCENGINE_API_KEY","DOUBAO_API_KEY")VOLC_BASE_URL  = os.getenv("VOLC_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")EMBED_MODEL    = os.getenv("EMBED_MODEL", "doubao-embedding-large-text-250515")DEFAULT_TIMEOUT = int(os.getenv("OPENAI_TIMEOUT", "120"))DEFAULT_MAX_RETRIES = int(os.getenv("OPENAI_MAX_RETRIES", "3"))CONNECT_TIMEOUT = float(os.getenv("CONNECT_TIMEOUT", "10"))READ_TIMEOUT = float(os.getenv("READ_TIMEOUT", "90"))WRITE_TIMEOUT = float(os.getenv("WRITE_TIMEOUT", "90"))POOL_TIMEOUT = float(os.getenv("POOL_TIMEOUT", "90"))USE_OFFLINE_EMBEDDING = os.getenv("USE_OFFLINE_EMBEDDING", "0") == "1"OFFLINE_EMBED_MODEL = os.getenv("OFFLINE_EMBED_MODEL", "paraphrase-multilingual-MiniLM-L12-v2")def _require(name:str, value:Optional[str]):    if not value:        raise RuntimeError(f"[password2] 缺少必要环境变量：{name}")# DEBUG 时做 TCP 连通性探针def _probe_tcp(host: str, port: int = 443, timeout: float = 3.0) -> bool:    if not DEBUG_PASSWORD2:        return True    try:        import socket        with socket.create_connection((host, port), timeout=timeout):            print(f"[debug] TCP OK: {host}:{port}")            return True    except Exception as e:        print(f"[debug] TCP FAIL: {host}:{port} -> {e}")        return False#LLM Encapsulationclass LLM:    def __init__(self,                 api_key:Optional[str]=None,                 base_url:Optional[str]=None,                 model: Optional[str]=None,                 max_retries:int = DEFAULT_MAX_RETRIES,                 timeout:int=DEFAULT_TIMEOUT):                self.api_key = api_key or DEEPSEEK_API_KEY                self.base_url = base_url or DEEPSEEK_BASE_URL                self.model = model or DEEPSEEK_MODEL                _require("DEEPSEEK_API_KEY/OPENAI_API_KEY",self.api_key)                #Certain of the version for openai only can read OPENAI_API_KEY                if not os.getenv("OPENAI_API_KEY"):                    os.environ["OPENAI_API_KEY"] = self.api_key                t = Timeout(connect=CONNECT_TIMEOUT, read=READ_TIMEOUT, write=WRITE_TIMEOUT,                            pool=POOL_TIMEOUT) if Timeout else timeout                # construct client(without Network usage)                self.client = OpenAI(                    api_key=self.api_key,                    base_url=self.base_url,                    max_retries=max_retries,                    timeout=t                )    def invoke(self,               prompt:str,               system:Optional[str]=None,               temperature:float=0.3,               stop:Optional[List[str]]=None,               )->str:                messages=[]                if system:                    messages.append({"role":"system","content":system})                messages.append({"role": "user","content": prompt})                resp = self.client.chat.completions.create(                    model = self.model,                    messages=messages,                    temperature=temperature,                    stop=stop,                    stream = False,                )                return resp.choices[0].message.content or ""#Embedding Encapsulationclass Embeddings:    """    providing :        embed_query(text:str) ->[float]        embed_documents(texts:Iterable[str] ->List{list[float]]    """    def __init__(self,                 api_key:Optional[str]=None,                 base_url:Optional[str]=None,                 model:Optional[str]=None,                 max_retries:int=DEFAULT_MAX_RETRIES,                 timeout:int=DEFAULT_TIMEOUT                 ):                    if USE_OFFLINE_EMBEDDING:                        try:                            from sentence_transformers import SentenceTransformer                            self._local = SentenceTransformer(OFFLINE_EMBED_MODEL)                            if DEBUG_PASSWORD2:                                print("[debug] using offline embedding:", OFFLINE_EMBED_MODEL)                                self.embed_query = lambda text: self._local.encode([text])[0].tolist()                                self.embed_documents = lambda texts: self._local.encode(list(texts)).tolist()                                return                        except Exception as e:                            raise RuntimeError(                                f"[password2] 启用离线嵌入失败：{e}，请先 pip install sentence-transformers 或关闭 USE_OFFLINE_EMBEDDING")                    self.api_key = api_key or VOLC_API_KEY                    self.base_url = base_url or VOLC_BASE_URL                    self.model= model or EMBED_MODEL                    _require("VOLC_API_KEY", self.api_key)                    # DEBUG: 简单 TCP 连通性探针（可快速判断是网络问题还是 API 问题）                    try:                        from urllib.parse import urlparse                        parsed = urlparse(self.base_url)                        _probe_tcp(parsed.hostname or "ark.cn-beijing.volces.com", 443, 3.0)                    except Exception:                        pass                    if not os.getenv("OPENAI_API_KEY"):                        os.environ["OPENAI_API_KEY"]= self.api_key                    t = Timeout(connect=CONNECT_TIMEOUT, read=READ_TIMEOUT, write=WRITE_TIMEOUT,                                pool=POOL_TIMEOUT) if Timeout else timeout                    self.client = OpenAI(                        api_key=self.api_key,                        base_url=self.base_url,                        max_retries=max_retries,                        timeout=t,                    )    def embed_query(self,text:str) -> List[float]:        resp = self.client.embeddings.create(            model=self.model,            input=[text],            encoding_format="float"        )        return resp.data[0].embedding    def embed_documents(self,texts: Iterable[str]) -> List[List[float]]:        resp = self.client.embeddings.create(            model=self.model,            input=list(texts),            encoding_format="float"        )        return [item.embedding for item in resp.data]#Factory functiondef load_llm() -> LLM:    return LLM()def load_embed() -> Embeddings:    return Embeddings()__all__ = ["LLM","Embeddings","load_llm","load_embed",]"""# 可选：直接运行本文件时做一次自检（不会在 import 时执行）if __name__ == "__main__":    print("[self-test] DEEPSEEK_API_KEY:", bool(DEEPSEEK_API_KEY))    print("[self-test] VOLC_API_KEY    :", bool(VOLC_API_KEY))    if DEEPSEEK_API_KEY and VOLC_API_KEY:        llm = load_llm()        emb = load_embed()        print("[self-test] LLM:", llm.invoke("只回答：pong"))        vec = emb.embed_query("hello")        print("[self-test] embedding dims:", len(vec))"""